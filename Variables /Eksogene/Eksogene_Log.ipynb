{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Endogenous variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdDNh266CQz"
      },
      "source": [
        "## NO - EA Inflation Core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbIKoniy3pLt",
        "outputId": "7eb680cd-cd73-469d-994d-7645c58d1bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norway core-HICP (TOT_X_NRG_FOOD) unit: I15\n",
            "Euro area HICP total (CP00) unit: I15\n",
            "            hicp_no_core  hicp_ea_all  p_no_core_log  p_ea_all_log\n",
            "time                                                              \n",
            "1999-12-01          79.9        75.09       4.380776      4.318687\n",
            "2000-01-01          79.7        75.13       4.378270      4.319220\n",
            "2000-02-01          80.1        75.37       4.383276      4.322409\n",
            "2000-03-01          80.3        75.60       4.385770      4.325456\n",
            "2000-04-01          80.8        75.67       4.391977      4.326382\n",
            "            hicp_no_core  hicp_ea_all  p_no_core_log  p_ea_all_log\n",
            "time                                                              \n",
            "2025-08-01         133.6       129.31       4.894850      4.862213\n",
            "2025-09-01         134.1       129.43       4.898586      4.863140\n",
            "2025-10-01         134.1       129.70       4.898586      4.865224\n",
            "2025-11-01         134.1       129.70       4.898586      4.865224\n",
            "2025-12-01         134.1       129.70       4.898586      4.865224\n",
            "Number of months: 313  | Period: 1999-12-01 → 2025-12-01\n"
          ]
        }
      ],
      "source": [
        "URL = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/prc_hicp_midx?format=TSV&compressed=true\"\n",
        "\n",
        "r = requests.get(URL, timeout=120)\n",
        "r.raise_for_status()\n",
        "with gzip.open(io.BytesIO(r.content), 'rt') as f:\n",
        "    df_raw = pd.read_csv(f, sep='\\t')\n",
        "\n",
        "# Split the dimension column: 'freq,unit,coicop,geo\\\\TIME_PERIOD'\n",
        "dimcol = df_raw.columns[0]\n",
        "df_raw[['freq','unit','coicop','geo']] = df_raw[dimcol].str.split(',', expand=True)\n",
        "df_raw = df_raw.drop(columns=[dimcol])\n",
        "\n",
        "# Identify time columns (YYYY-MM with optional trailing space)\n",
        "time_cols = [c for c in df_raw.columns if re.match(r'^\\d{4}-\\d{2}\\s*$', c)]\n",
        "df_long = df_raw.melt(\n",
        "    id_vars=['freq','unit','coicop','geo'],\n",
        "    value_vars=time_cols,\n",
        "    var_name='time',\n",
        "    value_name='value'\n",
        ")\n",
        "\n",
        "# Clean values: remove ':' and keep numeric values only\n",
        "df_long['value'] = df_long['value'].astype(str).str.strip()\n",
        "df_long = df_long[(df_long['value'] != ':') & (df_long['value'] != '')]\n",
        "num = df_long['value'].str.extract(r'^\\s*([-]?\\d+(?:\\.\\d+)?)')\n",
        "df_long = df_long[~num[0].isna()].copy()\n",
        "df_long['value'] = num[0].astype(float)\n",
        "\n",
        "# Standardize time format\n",
        "df_long['time'] = df_long['time'].str.strip()\n",
        "df_long['time'] = pd.to_datetime(df_long['time'], format=\"%Y-%m\", errors='coerce')\n",
        "df_long = df_long.dropna(subset=['time'])\n",
        "\n",
        "# Helper functions\n",
        "def best_unit(df, prefer=('I15','I05','I96')):\n",
        "    units = df['unit'].dropna().unique().tolist()\n",
        "    for u in prefer:\n",
        "        if u in units:\n",
        "            return u\n",
        "    return units[0] if units else None\n",
        "\n",
        "def get_series(df, geo, coicop, start_year=1999, end_year=2025):\n",
        "    sub = df.query(\"geo == @geo and coicop == @coicop\").copy()\n",
        "    if sub.empty:\n",
        "        raise ValueError(f\"No rows found for geo={geo}, coicop={coicop}\")\n",
        "    u = best_unit(sub)\n",
        "    sub = sub[sub['unit'] == u].copy()\n",
        "    sub = sub[(sub['time'].dt.year >= start_year) & (sub['time'].dt.year <= end_year)]\n",
        "    sub = sub.sort_values('time').reset_index(drop=True)\n",
        "    return sub[['time','value']].assign(unit=u)\n",
        "\n",
        "# Extract HICP series\n",
        "no_core = get_series(df_long, geo='NO', coicop='TOT_X_NRG_FOOD', start_year=1999, end_year=2025)\n",
        "print(f\"Norway core-HICP (TOT_X_NRG_FOOD) unit: {no_core['unit'].iloc[0]}\")\n",
        "\n",
        "ea_all = get_series(df_long, geo='EA', coicop='CP00', start_year=1999, end_year=2025)\n",
        "print(f\"Euro area HICP total (CP00) unit: {ea_all['unit'].iloc[0]}\")\n",
        "\n",
        "# Build complete monthly range and forward-fill\n",
        "full_index = pd.date_range(start=\"1999-12-01\", end=\"2025-12-01\", freq=\"MS\")\n",
        "\n",
        "df = full_index.to_frame(name=\"time\")\n",
        "df = df.merge(no_core[['time','value']].rename(columns={'value':'hicp_no_core'}), on='time', how='left')\n",
        "df = df.merge(ea_all[['time','value']].rename(columns={'value':'hicp_ea_all'}), on='time', how='left')\n",
        "df[['hicp_no_core','hicp_ea_all']] = df[['hicp_no_core','hicp_ea_all']].ffill()\n",
        "\n",
        "# Log transforms\n",
        "df['p_no_core_log'] = np.log(df['hicp_no_core'])\n",
        "df['p_ea_all_log']  = np.log(df['hicp_ea_all'])\n",
        "\n",
        "# Final monthly dataset\n",
        "out = df[['time', 'hicp_no_core', 'hicp_ea_all', 'p_no_core_log', 'p_ea_all_log']].copy()\n",
        "out = out.set_index('time')\n",
        "out.index.freq = 'MS'\n",
        "\n",
        "print(out.head(5))\n",
        "print(out.tail(5))\n",
        "print(f\"Number of months: {len(out)}  | Period: {out.index.min().date()} → {out.index.max().date()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99aZz9Z76As7"
      },
      "source": [
        "## EU NOK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWlO0n1D5_3c",
        "outputId": "1d273435-be8a-48d4-c1f8-eb24dfa478b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK\n",
            "DATE               \n",
            "2000-01-03   8.0620\n",
            "2000-01-04   8.1500\n",
            "2000-01-05   8.2060\n",
            "2000-01-06   8.2030\n",
            "2000-01-07   8.1945\n",
            "2000-01-08   8.1945\n",
            "2000-01-09   8.1945\n",
            "2000-01-10   8.1900\n",
            "2000-01-11   8.2075\n",
            "2000-01-12   8.2160\n",
            "            EUR_NOK\n",
            "DATE               \n",
            "2025-10-22  11.6430\n",
            "2025-10-23  11.5830\n",
            "2025-10-24  11.6185\n",
            "2025-10-25  11.6185\n",
            "2025-10-26  11.6185\n",
            "2025-10-27  11.6320\n",
            "2025-10-28  11.6335\n",
            "2025-10-29  11.6385\n",
            "2025-10-30  11.6648\n",
            "2025-10-31  11.6485\n",
            "\n",
            "Number of days: 9434  | Period: 2000-01-03 → 2025-10-31\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1) Download data from Norges Bank (2000–2025) ===\n",
        "url = (\"https://data.norges-bank.no/api/data/EXR/B.EUR.NOK.SP\"\n",
        "       \"?format=csv&bom=include&apisrc=nbi\"\n",
        "       \"&startPeriod=2000-01-01&endPeriod=2025-12-31&locale=no\")\n",
        "\n",
        "# Read CSV (semicolon separator, comma as decimal)\n",
        "df = pd.read_csv(url, sep=';', encoding='utf-8-sig', decimal=',')\n",
        "\n",
        "# === 2) Select date and exchange rate columns ===\n",
        "# Handles potential column name variations (e.g. OBS_VALUE_N)\n",
        "value_col = 'OBS_VALUE' if 'OBS_VALUE' in df.columns else 'OBS_VALUE_N'\n",
        "df = (\n",
        "    df[['TIME_PERIOD', value_col]]\n",
        "      .rename(columns={'TIME_PERIOD': 'DATE', value_col: 'EUR_NOK'})\n",
        ")\n",
        "\n",
        "df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
        "df = df.dropna(subset=['DATE','EUR_NOK']).sort_values('DATE').set_index('DATE')\n",
        "\n",
        "# === 3) Create full daily index and forward-fill missing values ===\n",
        "full_idx = pd.date_range(df.index.min(), df.index.max(), freq='D')\n",
        "df_daily = df.reindex(full_idx)\n",
        "\n",
        "df_daily['EUR_NOK'] = df_daily['EUR_NOK'].ffill()\n",
        "df_daily.index.name = 'DATE'\n",
        "df_daily = df_daily.asfreq('D')\n",
        "\n",
        "# === 4) Inspect output ===\n",
        "print(df_daily.head(10))\n",
        "print(df_daily.tail(10))\n",
        "print(f\"\\nNumber of days: {len(df_daily)}  | Period: {df_daily.index.min().date()} → {df_daily.index.max().date()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjv4PbgE6WbA"
      },
      "source": [
        "## Merge EU/NOK price with Inflation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mG0salU6aKG",
        "outputId": "0bf70e90-9a39-42be-cdbe-14799ab71bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK  hicp_no_core  hicp_ea_all  p_no_core_log  p_ea_all_log  \\\n",
            "DATE                                                                          \n",
            "2000-01-03   8.0620          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-04   8.1500          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-05   8.2060          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-06   8.2030          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-07   8.1945          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-08   8.1945          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-09   8.1945          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-10   8.1900          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-11   8.2075          79.7        75.13        4.37827       4.31922   \n",
            "2000-01-12   8.2160          79.7        75.13        4.37827       4.31922   \n",
            "\n",
            "            s_eurnok_log  \n",
            "DATE                      \n",
            "2000-01-03      2.087162  \n",
            "2000-01-04      2.098018  \n",
            "2000-01-05      2.104866  \n",
            "2000-01-06      2.104500  \n",
            "2000-01-07      2.103463  \n",
            "2000-01-08      2.103463  \n",
            "2000-01-09      2.103463  \n",
            "2000-01-10      2.102914  \n",
            "2000-01-11      2.105048  \n",
            "2000-01-12      2.106083  \n",
            "            EUR_NOK  hicp_no_core  hicp_ea_all  p_no_core_log  p_ea_all_log  \\\n",
            "DATE                                                                          \n",
            "2025-10-22  11.6430         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-23  11.5830         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-24  11.6185         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-25  11.6185         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-26  11.6185         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-27  11.6320         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-28  11.6335         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-29  11.6385         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-30  11.6648         134.1        129.7       4.898586      4.865224   \n",
            "2025-10-31  11.6485         134.1        129.7       4.898586      4.865224   \n",
            "\n",
            "            s_eurnok_log  \n",
            "DATE                      \n",
            "2025-10-22      2.454705  \n",
            "2025-10-23      2.449539  \n",
            "2025-10-24      2.452599  \n",
            "2025-10-25      2.452599  \n",
            "2025-10-26      2.452599  \n",
            "2025-10-27      2.453760  \n",
            "2025-10-28      2.453889  \n",
            "2025-10-29      2.454319  \n",
            "2025-10-30      2.456576  \n",
            "2025-10-31      2.455177  \n",
            "\n",
            "Number of days: 9434 | Period: 2000-01-03 → 2025-10-31\n"
          ]
        }
      ],
      "source": [
        "# === 1) Convert HICP (out) to daily frequency using forward-fill ===\n",
        "# Start and end dates are defined from 'out'\n",
        "daily_idx = pd.date_range(start=out.index.min(), end=out.index.max(), freq='D')\n",
        "\n",
        "out_daily = (\n",
        "    out.reindex(daily_idx)     # add all daily timestamps\n",
        "       .ffill()                # keep the last monthly value until a new month begins\n",
        ")\n",
        "\n",
        "out_daily.index.name = 'DATE'\n",
        "\n",
        "# === 2) Merge with Norges Bank daily data (df_daily) ===\n",
        "merged = (\n",
        "    df_daily[['EUR_NOK']]\n",
        "    .merge(out_daily, left_index=True, right_index=True, how='left')\n",
        ")\n",
        "\n",
        "# === 3) Add log of the exchange rate ===\n",
        "merged['s_eurnok_log'] = np.log(merged['EUR_NOK'])\n",
        "\n",
        "# === 4) Inspect output ===\n",
        "print(merged.head(10))\n",
        "print(merged.tail(10))\n",
        "print(f\"\\nNumber of days: {len(merged)} | Period: {merged.index.min().date()} → {merged.index.max().date()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A2gCOdQ7AJ9"
      },
      "source": [
        "## Reell valutakurs (q) & Inflasjonsdifferanse (dπ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee6PJ3w78p_Z",
        "outputId": "ea9528c0-6502-4a16-861d-3f4d03405429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK  hicp_no_core  hicp_ea_all        st        pt   pt_star  \\\n",
            "DATE                                                                           \n",
            "2000-01-31   8.0825          79.7        75.13  2.089701  4.378270  4.319220   \n",
            "2000-02-01   8.0730          80.1        75.37  2.088525  4.383276  4.322409   \n",
            "2000-02-02   8.0175          80.1        75.37  2.081627  4.383276  4.322409   \n",
            "2000-02-03   8.0475          80.1        75.37  2.085361  4.383276  4.322409   \n",
            "2000-02-04   8.0830          80.1        75.37  2.089763  4.383276  4.322409   \n",
            "2000-02-05   8.0830          80.1        75.37  2.089763  4.383276  4.322409   \n",
            "2000-02-06   8.0830          80.1        75.37  2.089763  4.383276  4.322409   \n",
            "2000-02-07   8.0590          80.1        75.37  2.086789  4.383276  4.322409   \n",
            "2000-02-08   8.0720          80.1        75.37  2.088401  4.383276  4.322409   \n",
            "2000-02-09   8.0825          80.1        75.37  2.089701  4.383276  4.322409   \n",
            "2000-02-10   8.0695          80.1        75.37  2.088092  4.383276  4.322409   \n",
            "2000-02-11   8.0395          80.1        75.37  2.084367  4.383276  4.322409   \n",
            "\n",
            "                  qt      pi_t  pi_t_star     dpi_t  \n",
            "DATE                                                 \n",
            "2000-01-31  2.030652  0.005006   0.003189  0.001817  \n",
            "2000-02-01  2.027659  0.005006   0.003189  0.001817  \n",
            "2000-02-02  2.020760  0.005006   0.003189  0.001817  \n",
            "2000-02-03  2.024495  0.005006   0.003189  0.001817  \n",
            "2000-02-04  2.028897  0.005006   0.003189  0.001817  \n",
            "2000-02-05  2.028897  0.005006   0.003189  0.001817  \n",
            "2000-02-06  2.028897  0.005006   0.003189  0.001817  \n",
            "2000-02-07  2.025923  0.005006   0.003189  0.001817  \n",
            "2000-02-08  2.027535  0.005006   0.003189  0.001817  \n",
            "2000-02-09  2.028835  0.005006   0.003189  0.001817  \n",
            "2000-02-10  2.027225  0.005006   0.003189  0.001817  \n",
            "2000-02-11  2.023500  0.005006   0.003189  0.001817  \n",
            "            EUR_NOK  hicp_no_core  hicp_ea_all        st        pt   pt_star  \\\n",
            "DATE                                                                           \n",
            "2025-10-20  11.7415         134.1        129.7  2.463130  4.898586  4.865224   \n",
            "2025-10-21  11.6693         134.1        129.7  2.456961  4.898586  4.865224   \n",
            "2025-10-22  11.6430         134.1        129.7  2.454705  4.898586  4.865224   \n",
            "2025-10-23  11.5830         134.1        129.7  2.449539  4.898586  4.865224   \n",
            "2025-10-24  11.6185         134.1        129.7  2.452599  4.898586  4.865224   \n",
            "2025-10-25  11.6185         134.1        129.7  2.452599  4.898586  4.865224   \n",
            "2025-10-26  11.6185         134.1        129.7  2.452599  4.898586  4.865224   \n",
            "2025-10-27  11.6320         134.1        129.7  2.453760  4.898586  4.865224   \n",
            "2025-10-28  11.6335         134.1        129.7  2.453889  4.898586  4.865224   \n",
            "2025-10-29  11.6385         134.1        129.7  2.454319  4.898586  4.865224   \n",
            "2025-10-30  11.6648         134.1        129.7  2.456576  4.898586  4.865224   \n",
            "2025-10-31  11.6485         134.1        129.7  2.455177  4.898586  4.865224   \n",
            "\n",
            "                  qt  pi_t  pi_t_star     dpi_t  \n",
            "DATE                                             \n",
            "2025-10-20  2.429768   0.0   0.002084 -0.002084  \n",
            "2025-10-21  2.423600   0.0   0.002084 -0.002084  \n",
            "2025-10-22  2.421343   0.0   0.002084 -0.002084  \n",
            "2025-10-23  2.416177   0.0   0.002084 -0.002084  \n",
            "2025-10-24  2.419237   0.0   0.002084 -0.002084  \n",
            "2025-10-25  2.419237   0.0   0.002084 -0.002084  \n",
            "2025-10-26  2.419237   0.0   0.002084 -0.002084  \n",
            "2025-10-27  2.420398   0.0   0.002084 -0.002084  \n",
            "2025-10-28  2.420527   0.0   0.002084 -0.002084  \n",
            "2025-10-29  2.420957   0.0   0.002084 -0.002084  \n",
            "2025-10-30  2.423214   0.0   0.002084 -0.002084  \n",
            "2025-10-31  2.421816   0.0   0.002084 -0.002084  \n",
            "\n",
            "Number of days (original): 9434 | Period: 2000-01-03 → 2025-10-31\n",
            "Number of days (after NaN removal): 9406\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === 1) Convert HICP (out) to daily frequency using forward-fill ===\n",
        "daily_idx = pd.date_range(start=out.index.min(), end=out.index.max(), freq='D')\n",
        "out_daily = (\n",
        "    out.reindex(daily_idx)      # add all daily dates\n",
        "       .ffill()                 # keep the last monthly value until next month\n",
        ")\n",
        "out_daily.index.name = 'DATE'\n",
        "\n",
        "# === 2) Merge with Norges Bank daily data (df_daily) ===\n",
        "merged = (\n",
        "    df_daily[['EUR_NOK']]\n",
        "    .merge(out_daily, left_index=True, right_index=True, how='left')\n",
        ")\n",
        "\n",
        "# === 3) Daily log levels and real exchange rate ===\n",
        "merged['st'] = np.log(merged['EUR_NOK'])                # log(EUR/NOK)\n",
        "merged['pt'] = np.log(merged['hicp_no_core'])           # Norway price level\n",
        "merged['pt_star'] = np.log(merged['hicp_ea_all'])       # Euro area price level\n",
        "merged['qt'] = merged['st'] - (merged['pt'] - merged['pt_star'])  # real exchange rate\n",
        "\n",
        "# === 4) Month-to-month inflation (Δlog) at month-end, mapped to daily within same month ===\n",
        "no_me = merged['hicp_no_core'].resample('M').last()\n",
        "ea_me = merged['hicp_ea_all'].resample('M').last()\n",
        "\n",
        "p_no_me = np.log(no_me)\n",
        "p_ea_me = np.log(ea_me)\n",
        "pi_no_m = p_no_me.diff(1)          # π_t  (Norway)\n",
        "pi_ea_m = p_ea_me.diff(1)          # π_t* (Euro area)\n",
        "dpi_m   = pi_no_m - pi_ea_m        # inflation differential\n",
        "\n",
        "# Map monthly values to daily so that the same value holds for the entire month\n",
        "didx   = merged.index\n",
        "end_me = didx.max() + pd.offsets.MonthEnd(0)\n",
        "drng   = pd.date_range(pi_no_m.index.min(), end_me, freq='D')\n",
        "\n",
        "def to_daily_same_month(s, daily_index):\n",
        "    daily_full = s.reindex(drng).bfill()  # backfill to fill same month\n",
        "    return daily_full.reindex(daily_index)\n",
        "\n",
        "merged['pi_t']      = to_daily_same_month(pi_no_m, didx)\n",
        "merged['pi_t_star'] = to_daily_same_month(pi_ea_m, didx)\n",
        "merged['dpi_t']     = to_daily_same_month(dpi_m, didx)\n",
        "\n",
        "# === 5) Clean for model use ===\n",
        "needed_cols = [\n",
        "    'EUR_NOK', 'hicp_no_core', 'hicp_ea_all',\n",
        "    'st', 'pt', 'pt_star', 'qt',\n",
        "    'pi_t', 'pi_t_star', 'dpi_t'\n",
        "]\n",
        "final_df = merged.dropna(subset=needed_cols).copy()\n",
        "\n",
        "# === 6) Display summary ===\n",
        "print(final_df[needed_cols].head(12))\n",
        "print(final_df[needed_cols].tail(12))\n",
        "print(f\"\\nNumber of days (original): {len(merged)} | Period: {merged.index.min().date()} → {merged.index.max().date()}\")\n",
        "print(f\"Number of days (after NaN removal): {len(final_df)}\")\n",
        "\n",
        "# === 7) Save dataset for modeling ===\n",
        "# final_df.to_csv('merged_q_inflation_ready.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNBmLjY7Gqsc",
        "outputId": "3d627ef2-3154-41db-f9e9-e12c9e6c2c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK         Q      d_pi\n",
            "DATE                                   \n",
            "2000-01-31   8.0825  2.030652  0.001817\n",
            "2000-02-01   8.0730  2.027659  0.001817\n",
            "2000-02-02   8.0175  2.020760  0.001817\n",
            "2000-02-03   8.0475  2.024495  0.001817\n",
            "2000-02-04   8.0830  2.028897  0.001817\n",
            "2000-02-05   8.0830  2.028897  0.001817\n",
            "2000-02-06   8.0830  2.028897  0.001817\n",
            "2000-02-07   8.0590  2.025923  0.001817\n",
            "2000-02-08   8.0720  2.027535  0.001817\n",
            "2000-02-09   8.0825  2.028835  0.001817\n",
            "2000-02-10   8.0695  2.027225  0.001817\n",
            "2000-02-11   8.0395  2.023500  0.001817\n",
            "            EUR_NOK         Q      d_pi\n",
            "DATE                                   \n",
            "2025-10-20  11.7415  2.429768 -0.002084\n",
            "2025-10-21  11.6693  2.423600 -0.002084\n",
            "2025-10-22  11.6430  2.421343 -0.002084\n",
            "2025-10-23  11.5830  2.416177 -0.002084\n",
            "2025-10-24  11.6185  2.419237 -0.002084\n",
            "2025-10-25  11.6185  2.419237 -0.002084\n",
            "2025-10-26  11.6185  2.419237 -0.002084\n",
            "2025-10-27  11.6320  2.420398 -0.002084\n",
            "2025-10-28  11.6335  2.420527 -0.002084\n",
            "2025-10-29  11.6385  2.420957 -0.002084\n",
            "2025-10-30  11.6648  2.423214 -0.002084\n",
            "2025-10-31  11.6485  2.421816 -0.002084\n",
            "\n",
            "Total rows: 9434  | Rows in final_small (no NaN): 9406\n"
          ]
        }
      ],
      "source": [
        "# === EUR/NOK, Q, and d_pi only ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Q = real exchange rate = s_t - (p_t - p_t*)\n",
        "merged['st']      = np.log(merged['EUR_NOK'])          # s_t\n",
        "merged['pt']      = np.log(merged['hicp_no_core'])     # p_t\n",
        "merged['pt_star'] = np.log(merged['hicp_ea_all'])      # p_t*\n",
        "merged['Q']       = merged['st'] - (merged['pt'] - merged['pt_star'])\n",
        "\n",
        "# 2) d_pi = inflation differential = π_t - π_t*\n",
        "#    (Δlog of end-of-month levels, mapped to daily within the same month)\n",
        "no_me = merged['hicp_no_core'].resample('M').last()\n",
        "ea_me = merged['hicp_ea_all'].resample('M').last()\n",
        "\n",
        "pi_no_m = np.log(no_me).diff(1)\n",
        "pi_ea_m = np.log(ea_me).diff(1)\n",
        "d_pi_m  = pi_no_m - pi_ea_m\n",
        "\n",
        "# Map to daily frequency: backfill from month-end so value holds for the same month\n",
        "didx   = merged.index\n",
        "end_me = didx.max() + pd.offsets.MonthEnd(0)\n",
        "drng   = pd.date_range(d_pi_m.index.min(), end_me, freq='D')\n",
        "\n",
        "d_pi_daily = d_pi_m.reindex(drng).bfill().reindex(didx)\n",
        "merged['d_pi'] = d_pi_daily\n",
        "\n",
        "# 3) Final dataset for model\n",
        "final_small = merged[['EUR_NOK', 'Q', 'd_pi']].dropna().copy()\n",
        "\n",
        "# Optional: preview\n",
        "print(final_small.head(12))\n",
        "print(final_small.tail(12))\n",
        "print(f\"\\nTotal rows: {len(merged)}  | Rows in final_small (no NaN): {len(final_small)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvXxouuqHY0W"
      },
      "source": [
        "## Styringsrenter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IhmInbgHYDM",
        "outputId": "62469430-f345-4bc5-e580-5fba831a4368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            policy_rate\n",
            "DATE                   \n",
            "2000-01-03          5.5\n",
            "2000-01-04          5.5\n",
            "2000-01-05          5.5\n",
            "2000-01-06          5.5\n",
            "2000-01-07          5.5\n",
            "2000-01-08          5.5\n",
            "2000-01-09          5.5\n",
            "2000-01-10          5.5\n",
            "2000-01-11          5.5\n",
            "2000-01-12          5.5\n",
            "            policy_rate\n",
            "DATE                   \n",
            "2025-10-22          4.0\n",
            "2025-10-23          4.0\n",
            "2025-10-24          4.0\n",
            "2025-10-25          4.0\n",
            "2025-10-26          4.0\n",
            "2025-10-27          4.0\n",
            "2025-10-28          4.0\n",
            "2025-10-29          4.0\n",
            "2025-10-30          4.0\n",
            "2025-10-31          4.0\n",
            "\n",
            "policy_rate covers 2000-01-03 → 2025-10-31 | NaN count: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1) Fetch policy rate (Norges Bank API) ===\n",
        "url = (\n",
        "    \"https://data.norges-bank.no/api/data/IR/B.KPRA.SD.R\"\n",
        "    \"?apisrc=qb&format=csv&startPeriod=1996-01-01&endPeriod=2025-09-26&locale=no&bom=include\"\n",
        ")\n",
        "\n",
        "df_rate = pd.read_csv(url, sep=\";\", encoding=\"utf-8-sig\", engine=\"python\")\n",
        "df_rate.columns = [c.strip() for c in df_rate.columns]\n",
        "\n",
        "time_candidates  = [\"TIME_PERIOD\", \"Tid\", \"TIME\", \"Date\", \"PERIOD\"]\n",
        "value_candidates = [\"OBS_VALUE\", \"Observasjonsverdi\", \"Value\", \"VALUE\"]\n",
        "\n",
        "time_col  = next((c for c in time_candidates  if c in df_rate.columns), None)\n",
        "value_col = next((c for c in value_candidates if c in df_rate.columns), None)\n",
        "if time_col is None or value_col is None:\n",
        "    raise KeyError(f\"Could not find time/value columns. Columns: {df_rate.columns.tolist()}\")\n",
        "\n",
        "rate = df_rate[[time_col, value_col]].copy()\n",
        "rate.rename(columns={time_col: \"DATE\", value_col: \"policy_rate\"}, inplace=True)\n",
        "rate[\"policy_rate\"] = pd.to_numeric(rate[\"policy_rate\"].astype(str).str.replace(\",\", \".\"), errors=\"coerce\")\n",
        "\n",
        "rate[\"DATE\"] = pd.to_datetime(rate[\"DATE\"], errors=\"coerce\")\n",
        "rate = rate.dropna(subset=[\"DATE\", \"policy_rate\"]).sort_values(\"DATE\").set_index(\"DATE\")\n",
        "\n",
        "# === 2) Reindex to full daily calendar ===\n",
        "target_idx = pd.date_range(start=merged.index.min(), end=merged.index.max(), freq=\"D\")\n",
        "policy_rate_daily = rate.reindex(target_idx).ffill().bfill()\n",
        "policy_rate_daily.index.name = \"DATE\"\n",
        "\n",
        "# === 3) Add to merged dataset safely ===\n",
        "merged = merged.assign(policy_rate=policy_rate_daily['policy_rate'])\n",
        "\n",
        "# === 4) Inspect ===\n",
        "print(merged[['policy_rate']].head(10))\n",
        "print(merged[['policy_rate']].tail(10))\n",
        "print(f\"\\npolicy_rate covers {merged.index.min().date()} → {merged.index.max().date()} | \"\n",
        "      f\"NaN count: {merged['policy_rate'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJUhLW5YJAMW",
        "outputId": "1e4294ee-13f6-4bf0-be25-64a330abf8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            eu_policy_rate\n",
            "DATE                      \n",
            "1999-11-05             2.0\n",
            "1999-11-06             2.0\n",
            "1999-11-07             2.0\n",
            "1999-11-08             2.0\n",
            "1999-11-09             2.0\n",
            "1999-11-10             2.0\n",
            "1999-11-11             2.0\n",
            "1999-11-12             2.0\n",
            "1999-11-13             2.0\n",
            "1999-11-14             2.0\n",
            "1999-11-15             2.0\n",
            "1999-11-16             2.0\n",
            "            eu_policy_rate\n",
            "DATE                      \n",
            "2025-05-31            2.25\n",
            "2025-06-01            2.25\n",
            "2025-06-02            2.25\n",
            "2025-06-03            2.25\n",
            "2025-06-04            2.25\n",
            "2025-06-05            2.25\n",
            "2025-06-06            2.25\n",
            "2025-06-07            2.25\n",
            "2025-06-08            2.25\n",
            "2025-06-09            2.25\n",
            "2025-06-10            2.25\n",
            "2025-06-11            2.00\n",
            "\n",
            "EU policy rate daily: 1999-11-05 → 2025-06-11 | NaN: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1) Fetch HTML table \"Key ECB interest rates\"\n",
        "# -------------------------------------------------------\n",
        "URL = \"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/key_ecb_interest_rates/html/index.en.html\"\n",
        "html = requests.get(URL, timeout=30).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "# Find table containing \"Date (with effect from)\"\n",
        "target_table = None\n",
        "for tbl in soup.find_all(\"table\"):\n",
        "    if tbl.find(string=re.compile(r\"Date \\(with effect from\\)\", re.I)):\n",
        "        target_table = tbl\n",
        "        break\n",
        "if target_table is None:\n",
        "    raise RuntimeError(\"Table with 'Date (with effect from)' not found on ECB page.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 2) Extract rows and parse year, date, and rates\n",
        "# -------------------------------------------------------\n",
        "def clean_text(x: str) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    x = re.sub(r\"\\s+\", \" \", x.strip())\n",
        "    x = x.replace(\"−\", \"-\")  # minus sign\n",
        "    x = re.sub(r\"\\^\\{\\d+\\}\", \"\", x)  # remove superscripts\n",
        "    return x\n",
        "\n",
        "def to_float_or_nan(s: str):\n",
        "    s = s.strip()\n",
        "    if s in (\"-\", \"\"):\n",
        "        return np.nan\n",
        "    s = s.replace(\",\", \".\")\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "MONTH_MAP = {\n",
        "    \"Jan.\": \"Jan\", \"Feb.\": \"Feb\", \"Mar.\": \"Mar\", \"Apr.\": \"Apr\", \"May\": \"May\",\n",
        "    \"Jun.\": \"Jun\", \"Jul.\": \"Jul\", \"Aug.\": \"Aug\", \"Sep.\": \"Sep\", \"Oct.\": \"Oct\",\n",
        "    \"Nov.\": \"Nov\", \"Dec.\": \"Dec\"\n",
        "}\n",
        "\n",
        "rows_out = []\n",
        "current_year = None\n",
        "\n",
        "for tr in target_table.find_all(\"tr\"):\n",
        "    tds = [clean_text(td.get_text(\" \", strip=True)) for td in tr.find_all([\"td\", \"th\"])]\n",
        "    if not tds:\n",
        "        continue\n",
        "\n",
        "    joined = \" \".join(tds)\n",
        "    if re.search(r\"Fixed rate tenders|Variable rate tenders|Minimum bid rate\", joined, re.I):\n",
        "        continue\n",
        "    if re.search(r\"Date \\(with effect from\\)\", joined, re.I):\n",
        "        continue\n",
        "\n",
        "    year, dstr, dep, mro, mlf = None, None, None, None, None\n",
        "\n",
        "    # Normal case: 5 columns [Year, Date, Deposit, MRO, MLF]\n",
        "    if len(tds) >= 5 and re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "        year, dstr, dep, mro, mlf = tds[0], tds[1], tds[2], tds[3], tds[4]\n",
        "    else:\n",
        "        # Handle cases where year/date are merged in one cell\n",
        "        found = None\n",
        "        for i, cell in enumerate(tds[:2]):\n",
        "            m = re.search(r\"(\\d{4})\\s+(\\d{1,2})\\s+([A-Za-z]{3}\\.?)\", cell)\n",
        "            if m:\n",
        "                found = (m.group(1), m.group(2), m.group(3), i)\n",
        "                break\n",
        "        if found:\n",
        "            year, day, mon_raw, idx = found\n",
        "            mon = MONTH_MAP.get(mon_raw if mon_raw.endswith(\".\") else mon_raw + \".\", mon_raw.replace(\".\", \"\"))\n",
        "            dstr = f\"{day} {mon}\"\n",
        "            rest = tds[idx + 1:]\n",
        "            if len(rest) >= 3:\n",
        "                dep, mro, mlf = rest[0], rest[1], rest[2]\n",
        "        else:\n",
        "            if len(tds) >= 4 and (re.fullmatch(r\"\\d{4}\", tds[0]) or current_year):\n",
        "                if re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "                    current_year = tds[0]\n",
        "                    dstr, dep, mro, mlf = tds[1], tds[2], tds[3], (tds[4] if len(tds) > 4 else \"-\")\n",
        "                else:\n",
        "                    year = current_year\n",
        "                    dstr, dep, mro, mlf = tds[0], tds[1], tds[2], (tds[3] if len(tds) > 3 else \"-\")\n",
        "\n",
        "    if year is None and re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "        year = tds[0]\n",
        "    if year is None:\n",
        "        year = current_year\n",
        "    else:\n",
        "        current_year = year\n",
        "\n",
        "    if not (year and dstr):\n",
        "        continue\n",
        "\n",
        "    parts = dstr.split()\n",
        "    if len(parts) >= 2:\n",
        "        day, mon_raw = parts[0], parts[1]\n",
        "        mon = MONTH_MAP.get(mon_raw, mon_raw.replace(\".\", \"\"))\n",
        "        d_iso = pd.to_datetime(f\"{day} {mon} {year}\", format=\"%d %b %Y\", errors=\"coerce\")\n",
        "    else:\n",
        "        d_iso = pd.NaT\n",
        "\n",
        "    dep_v = to_float_or_nan(dep or \"\")\n",
        "    mro_v = to_float_or_nan(mro or \"\")\n",
        "    mlf_v = to_float_or_nan(mlf or \"\")\n",
        "\n",
        "    rows_out.append({\n",
        "        \"effective_date\": d_iso,\n",
        "        \"deposit_facility\": dep_v,\n",
        "        \"main_refi\": mro_v,\n",
        "        \"marginal_lending\": mlf_v\n",
        "    })\n",
        "\n",
        "df_ecb = pd.DataFrame(rows_out).dropna(subset=[\"effective_date\"]).sort_values(\"effective_date\").reset_index(drop=True)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 3) Select policy rate (Deposit facility)\n",
        "# -------------------------------------------------------\n",
        "df_ecb[\"eu_policy_rate\"] = df_ecb[\"deposit_facility\"]\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4) Create daily time series with forward/backward fill\n",
        "# -------------------------------------------------------\n",
        "daily_start = df_ecb[\"effective_date\"].min()\n",
        "daily_end   = df_ecb[\"effective_date\"].max()\n",
        "\n",
        "daily_idx = pd.date_range(start=daily_start, end=daily_end, freq=\"D\")\n",
        "daily = (\n",
        "    df_ecb.set_index(\"effective_date\")[[\"eu_policy_rate\"]]\n",
        "          .reindex(daily_idx)\n",
        "          .ffill()\n",
        "          .bfill()\n",
        ")\n",
        "daily.index.name = \"DATE\"\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 5) (Optional) Merge into your dataset\n",
        "# -------------------------------------------------------\n",
        "# merged = merged.merge(daily, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 6) Inspect\n",
        "# -------------------------------------------------------\n",
        "print(daily.head(12))\n",
        "print(daily.tail(12))\n",
        "print(f\"\\nEU policy rate daily: {daily.index.min().date()} → {daily.index.max().date()} | NaN: {daily['eu_policy_rate'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPTMsAciJg9I",
        "outputId": "01ba0e0e-10be-462c-a75c-e02876a4f408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            eu_policy_rate\n",
            "DATE                      \n",
            "1999-11-05             2.0\n",
            "1999-11-06             2.0\n",
            "1999-11-07             2.0\n",
            "1999-11-08             2.0\n",
            "1999-11-09             2.0\n",
            "1999-11-10             2.0\n",
            "1999-11-11             2.0\n",
            "1999-11-12             2.0\n",
            "1999-11-13             2.0\n",
            "1999-11-14             2.0\n",
            "1999-11-15             2.0\n",
            "1999-11-16             2.0\n",
            "            eu_policy_rate\n",
            "DATE                      \n",
            "2025-05-31            2.25\n",
            "2025-06-01            2.25\n",
            "2025-06-02            2.25\n",
            "2025-06-03            2.25\n",
            "2025-06-04            2.25\n",
            "2025-06-05            2.25\n",
            "2025-06-06            2.25\n",
            "2025-06-07            2.25\n",
            "2025-06-08            2.25\n",
            "2025-06-09            2.25\n",
            "2025-06-10            2.25\n",
            "2025-06-11            2.00\n",
            "\n",
            "EU policy rate daily: 1999-11-05 → 2025-06-11 | NaN: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1) Fetch HTML table \"Key ECB interest rates\"\n",
        "# -------------------------------------------------------\n",
        "URL = \"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/key_ecb_interest_rates/html/index.en.html\"\n",
        "html = requests.get(URL, timeout=30).text\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "# Find table containing \"Date (with effect from)\"\n",
        "target_table = None\n",
        "for tbl in soup.find_all(\"table\"):\n",
        "    if tbl.find(string=re.compile(r\"Date \\(with effect from\\)\", re.I)):\n",
        "        target_table = tbl\n",
        "        break\n",
        "if target_table is None:\n",
        "    raise RuntimeError(\"Table with 'Date (with effect from)' not found on ECB page.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 2) Extract rows and parse year, date, and rates\n",
        "# -------------------------------------------------------\n",
        "def clean_text(x: str) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    x = re.sub(r\"\\s+\", \" \", x.strip())\n",
        "    x = x.replace(\"−\", \"-\")  # minus sign\n",
        "    x = re.sub(r\"\\^\\{\\d+\\}\", \"\", x)  # remove superscripts\n",
        "    return x\n",
        "\n",
        "def to_float_or_nan(s: str):\n",
        "    s = s.strip()\n",
        "    if s in (\"-\", \"\"):\n",
        "        return np.nan\n",
        "    s = s.replace(\",\", \".\")\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "MONTH_MAP = {\n",
        "    \"Jan.\": \"Jan\", \"Feb.\": \"Feb\", \"Mar.\": \"Mar\", \"Apr.\": \"Apr\", \"May\": \"May\",\n",
        "    \"Jun.\": \"Jun\", \"Jul.\": \"Jul\", \"Aug.\": \"Aug\", \"Sep.\": \"Sep\", \"Oct.\": \"Oct\",\n",
        "    \"Nov.\": \"Nov\", \"Dec.\": \"Dec\"\n",
        "}\n",
        "\n",
        "rows_out = []\n",
        "current_year = None\n",
        "\n",
        "for tr in target_table.find_all(\"tr\"):\n",
        "    tds = [clean_text(td.get_text(\" \", strip=True)) for td in tr.find_all([\"td\", \"th\"])]\n",
        "    if not tds:\n",
        "        continue\n",
        "\n",
        "    joined = \" \".join(tds)\n",
        "    if re.search(r\"Fixed rate tenders|Variable rate tenders|Minimum bid rate\", joined, re.I):\n",
        "        continue\n",
        "    if re.search(r\"Date \\(with effect from\\)\", joined, re.I):\n",
        "        continue\n",
        "\n",
        "    year, dstr, dep, mro, mlf = None, None, None, None, None\n",
        "\n",
        "    # Normal case: 5 columns [Year, Date, Deposit, MRO, MLF]\n",
        "    if len(tds) >= 5 and re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "        year, dstr, dep, mro, mlf = tds[0], tds[1], tds[2], tds[3], tds[4]\n",
        "    else:\n",
        "        # Handle cases where year/date are merged in one cell\n",
        "        found = None\n",
        "        for i, cell in enumerate(tds[:2]):\n",
        "            m = re.search(r\"(\\d{4})\\s+(\\d{1,2})\\s+([A-Za-z]{3}\\.?)\", cell)\n",
        "            if m:\n",
        "                found = (m.group(1), m.group(2), m.group(3), i)\n",
        "                break\n",
        "        if found:\n",
        "            year, day, mon_raw, idx = found\n",
        "            mon = MONTH_MAP.get(mon_raw if mon_raw.endswith(\".\") else mon_raw + \".\", mon_raw.replace(\".\", \"\"))\n",
        "            dstr = f\"{day} {mon}\"\n",
        "            rest = tds[idx + 1:]\n",
        "            if len(rest) >= 3:\n",
        "                dep, mro, mlf = rest[0], rest[1], rest[2]\n",
        "        else:\n",
        "            if len(tds) >= 4 and (re.fullmatch(r\"\\d{4}\", tds[0]) or current_year):\n",
        "                if re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "                    current_year = tds[0]\n",
        "                    dstr, dep, mro, mlf = tds[1], tds[2], tds[3], (tds[4] if len(tds) > 4 else \"-\")\n",
        "                else:\n",
        "                    year = current_year\n",
        "                    dstr, dep, mro, mlf = tds[0], tds[1], tds[2], (tds[3] if len(tds) > 3 else \"-\")\n",
        "\n",
        "    if year is None and re.fullmatch(r\"\\d{4}\", tds[0]):\n",
        "        year = tds[0]\n",
        "    if year is None:\n",
        "        year = current_year\n",
        "    else:\n",
        "        current_year = year\n",
        "\n",
        "    if not (year and dstr):\n",
        "        continue\n",
        "\n",
        "    parts = dstr.split()\n",
        "    if len(parts) >= 2:\n",
        "        day, mon_raw = parts[0], parts[1]\n",
        "        mon = MONTH_MAP.get(mon_raw, mon_raw.replace(\".\", \"\"))\n",
        "        d_iso = pd.to_datetime(f\"{day} {mon} {year}\", format=\"%d %b %Y\", errors=\"coerce\")\n",
        "    else:\n",
        "        d_iso = pd.NaT\n",
        "\n",
        "    dep_v = to_float_or_nan(dep or \"\")\n",
        "    mro_v = to_float_or_nan(mro or \"\")\n",
        "    mlf_v = to_float_or_nan(mlf or \"\")\n",
        "\n",
        "    rows_out.append({\n",
        "        \"effective_date\": d_iso,\n",
        "        \"deposit_facility\": dep_v,\n",
        "        \"main_refi\": mro_v,\n",
        "        \"marginal_lending\": mlf_v\n",
        "    })\n",
        "\n",
        "df_ecb = pd.DataFrame(rows_out).dropna(subset=[\"effective_date\"]).sort_values(\"effective_date\").reset_index(drop=True)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 3) Select policy rate (Deposit facility)\n",
        "# -------------------------------------------------------\n",
        "df_ecb[\"eu_policy_rate\"] = df_ecb[\"deposit_facility\"]\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4) Create daily time series with forward/backward fill\n",
        "# -------------------------------------------------------\n",
        "daily_start = df_ecb[\"effective_date\"].min()\n",
        "daily_end   = df_ecb[\"effective_date\"].max()\n",
        "\n",
        "daily_idx = pd.date_range(start=daily_start, end=daily_end, freq=\"D\")\n",
        "daily = (\n",
        "    df_ecb.set_index(\"effective_date\")[[\"eu_policy_rate\"]]\n",
        "          .reindex(daily_idx)\n",
        "          .ffill()\n",
        "          .bfill()\n",
        ")\n",
        "daily.index.name = \"DATE\"\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 5) (Optional) Merge into your dataset\n",
        "# -------------------------------------------------------\n",
        "# merged = merged.merge(daily, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 6) Inspect\n",
        "# -------------------------------------------------------\n",
        "print(daily.head(12))\n",
        "print(daily.tail(12))\n",
        "print(f\"\\nEU policy rate daily: {daily.index.min().date()} → {daily.index.max().date()} | NaN: {daily['eu_policy_rate'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afONLDZEKd02"
      },
      "source": [
        "## Daily Endogenous "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzlhOaaDJ_Uw",
        "outputId": "18afd5c0-3b94-4960-8b86-fcdfe16b7c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK         Q      d_pi  dI_t\n",
            "DATE                                         \n",
            "2000-01-31   8.0825  2.030652  0.001817   3.5\n",
            "2000-02-01   8.0730  2.027659  0.001817   3.5\n",
            "2000-02-02   8.0175  2.020760  0.001817   3.5\n",
            "2000-02-03   8.0475  2.024495  0.001817   3.5\n",
            "2000-02-04   8.0830  2.028897  0.001817   3.5\n",
            "2000-02-05   8.0830  2.028897  0.001817   3.5\n",
            "2000-02-06   8.0830  2.028897  0.001817   3.5\n",
            "2000-02-07   8.0590  2.025923  0.001817   3.5\n",
            "2000-02-08   8.0720  2.027535  0.001817   3.5\n",
            "2000-02-09   8.0825  2.028835  0.001817   3.5\n",
            "2000-02-10   8.0695  2.027225  0.001817   3.5\n",
            "2000-02-11   8.0395  2.023500  0.001817   3.5\n",
            "            EUR_NOK         Q      d_pi  dI_t\n",
            "DATE                                         \n",
            "2025-10-20  11.7415  2.429768 -0.002084   2.0\n",
            "2025-10-21  11.6693  2.423600 -0.002084   2.0\n",
            "2025-10-22  11.6430  2.421343 -0.002084   2.0\n",
            "2025-10-23  11.5830  2.416177 -0.002084   2.0\n",
            "2025-10-24  11.6185  2.419237 -0.002084   2.0\n",
            "2025-10-25  11.6185  2.419237 -0.002084   2.0\n",
            "2025-10-26  11.6185  2.419237 -0.002084   2.0\n",
            "2025-10-27  11.6320  2.420398 -0.002084   2.0\n",
            "2025-10-28  11.6335  2.420527 -0.002084   2.0\n",
            "2025-10-29  11.6385  2.420957 -0.002084   2.0\n",
            "2025-10-30  11.6648  2.423214 -0.002084   2.0\n",
            "2025-10-31  11.6485  2.421816 -0.002084   2.0\n",
            "\n",
            "Total rows: 9406 | Period: 2000-01-31 → 2025-10-31\n"
          ]
        }
      ],
      "source": [
        "# === Merge final_small with interest rate differential (left join) ===\n",
        "final_with_rates = (\n",
        "    final_small\n",
        "    .merge(rates[[\"dI_t\"]], left_index=True, right_index=True, how=\"left\")\n",
        "    .dropna(subset=[\"dI_t\"])  # keep only rows where interest rate differential is defined\n",
        ")\n",
        "\n",
        "# Select only required columns\n",
        "final_with_rates = final_with_rates[[\"EUR_NOK\", \"Q\", \"d_pi\", \"dI_t\"]]\n",
        "\n",
        "# --- Inspect ---\n",
        "print(final_with_rates.head(12))\n",
        "print(final_with_rates.tail(12))\n",
        "print(f\"\\nTotal rows: {len(final_with_rates)} | \"\n",
        "      f\"Period: {final_with_rates.index.min().date()} → {final_with_rates.index.max().date()}\")\n",
        "\n",
        "# (Optional) save\n",
        "# final_with_rates.to_csv(\"final_dataset_ready.csv\", index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZk-NCZKlgU"
      },
      "source": [
        "## Monthly Endogenous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5a49DSgKk3U",
        "outputId": "2fd8eaab-460f-4fc8-aa5a-cf576100dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             EUR_NOK         Q      d_pi  dI_t\n",
            "DATE                                          \n",
            "2000-01-31  8.082500  2.030652  0.001817  3.50\n",
            "2000-02-29  8.099034  2.028587  0.001817  3.50\n",
            "2000-03-31  8.112532  2.030130 -0.000553  3.50\n",
            "2000-04-30  8.149967  2.032116  0.005282  3.75\n",
            "2000-05-31  8.194226  2.052451 -0.001189  3.75\n",
            "2000-06-30  8.255183  2.037880  0.000017  4.25\n",
            "2000-07-31  8.175952  2.044609 -0.005020  4.25\n",
            "2000-08-31  8.096790  2.030095 -0.000788  4.75\n",
            "2000-09-30  8.030400  2.020539  0.003469  5.00\n",
            "2000-10-31  8.003935  2.001680 -0.000261  3.25\n",
            "2000-11-30  7.996533  2.023794  0.000365  3.25\n",
            "2000-12-31  8.143081  2.050894 -0.004871  3.25\n",
            "              EUR_NOK         Q      d_pi  dI_t\n",
            "DATE                                           \n",
            "2024-11-30  11.741250  2.420865  0.003233  1.25\n",
            "2024-12-31  11.755200  2.431128 -0.000508  1.50\n",
            "2025-01-31  11.746923  2.430318 -0.004094  1.50\n",
            "2025-02-28  11.658339  2.423597  0.005630  1.75\n",
            "2025-03-31  11.551219  2.399005 -0.002335  2.00\n",
            "2025-04-30  11.880120  2.429050  0.004064  2.25\n",
            "2025-05-31  11.614981  2.407774 -0.001698  2.25\n",
            "2025-06-30  11.584587  2.430786  0.002119  2.25\n",
            "2025-07-31  11.853419  2.424328  0.001332  2.25\n",
            "2025-08-31  11.859823  2.430918 -0.008928  2.25\n",
            "2025-09-30  11.672533  2.426406  0.002808  2.00\n",
            "2025-10-31  11.666274  2.421816 -0.002084  2.00\n",
            "\n",
            "Total rows (monthly): 310 | Period: 2000-01-31 → 2025-10-31\n"
          ]
        }
      ],
      "source": [
        "# === Aggregate to monthly frequency ===\n",
        "\n",
        "# Resample to month-end ('M' = month end)\n",
        "final_monthly = pd.DataFrame({\n",
        "    \"EUR_NOK\": final_with_rates[\"EUR_NOK\"].resample(\"M\").mean(),   # average exchange rate\n",
        "    \"Q\":       final_with_rates[\"Q\"].resample(\"M\").last(),         # real exchange rate (level)\n",
        "    \"d_pi\":    final_with_rates[\"d_pi\"].resample(\"M\").last(),      # inflation differential\n",
        "    \"dI_t\":    final_with_rates[\"dI_t\"].resample(\"M\").last(),      # interest rate differential\n",
        "})\n",
        "\n",
        "# Remove potential NaN rows at the beginning\n",
        "final_monthly = final_monthly.dropna()\n",
        "\n",
        "# --- Inspect ---\n",
        "print(final_monthly.head(12))\n",
        "print(final_monthly.tail(12))\n",
        "print(f\"\\nTotal rows (monthly): {len(final_monthly)} | \"\n",
        "      f\"Period: {final_monthly.index.min().date()} → {final_monthly.index.max().date()}\")\n",
        "\n",
        "# (Optional) save\n",
        "# final_monthly.to_csv(\"final_dataset_monthly.csv\", index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMJ2XjRUOrOF"
      },
      "source": [
        "# Exogenous Variables "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJKpbBtOto7"
      },
      "source": [
        "## VIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_Ouo17XOuq0",
        "outputId": "db3f90f4-9d5e-43ae-9fe1-04788c832341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              VIX\n",
            "DATE             \n",
            "2000-01-03  24.21\n",
            "2000-01-04  27.01\n",
            "2000-01-05  26.41\n",
            "2000-01-06  25.73\n",
            "2000-01-07  21.72\n",
            "2000-01-08  21.72\n",
            "2000-01-09  21.72\n",
            "2000-01-10  21.71\n",
            "2000-01-11  22.50\n",
            "2000-01-12  22.84\n",
            "              VIX\n",
            "DATE             \n",
            "2025-10-22  18.60\n",
            "2025-10-23  17.30\n",
            "2025-10-24  16.37\n",
            "2025-10-25  16.37\n",
            "2025-10-26  16.37\n",
            "2025-10-27  15.79\n",
            "2025-10-28  16.42\n",
            "2025-10-29  16.92\n",
            "2025-10-30  16.91\n",
            "2025-10-31  17.44\n",
            "\n",
            "VIX daily: 2000-01-03 → 2025-10-31 | NaN: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1) Load VIX (CBOE) ===\n",
        "url = \"https://cdn.cboe.com/api/global/us_indices/daily_prices/VIX_History.csv\"\n",
        "vix = pd.read_csv(url)\n",
        "\n",
        "# === 2) Standardize columns and pick the correct \"close\" column ===\n",
        "vix.columns = [c.strip().upper() for c in vix.columns]\n",
        "\n",
        "# candidates seen: CLOSE, VIX CLOSE, Close\n",
        "close_candidates = [\"CLOSE\", \"VIX CLOSE\", \"VIX_CLOSE\"]\n",
        "close_col = next((c for c in close_candidates if c in vix.columns), None)\n",
        "if close_col is None:\n",
        "    raise KeyError(f\"Could not find VIX close column. Available: {list(vix.columns)}\")\n",
        "\n",
        "# Build a clean VIX dataframe\n",
        "vix = vix.rename(columns={\"DATE\": \"DATE\"}).copy()\n",
        "vix[\"DATE\"] = pd.to_datetime(vix[\"DATE\"], errors=\"coerce\")\n",
        "vix[\"VIX\"] = pd.to_numeric(vix[close_col], errors=\"coerce\")\n",
        "vix = vix.dropna(subset=[\"DATE\", \"VIX\"]).sort_values(\"DATE\").set_index(\"DATE\")\n",
        "\n",
        "# === 3) Reindex to your daily calendar and fill gaps ===\n",
        "target_idx = pd.date_range(start=merged.index.min(), end=merged.index.max(), freq=\"D\")\n",
        "vix_daily = vix.reindex(target_idx).ffill().bfill()\n",
        "vix_daily.index.name = \"DATE\"\n",
        "\n",
        "# === 4) Add to main dataset safely (overwrite if exists) ===\n",
        "merged = merged.assign(VIX=vix_daily[\"VIX\"])\n",
        "\n",
        "# === 5) Inspect ===\n",
        "print(merged[[\"VIX\"]].head(10))\n",
        "print(merged[[\"VIX\"]].tail(10))\n",
        "print(f\"\\nVIX daily: {merged.index.min().date()} → {merged.index.max().date()} | NaN: {merged['VIX'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO-L8bPlQJl8"
      },
      "source": [
        "## Brent Oil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "gAtCO6kdQLKJ",
        "outputId": "7b7f51ff-c739-4fdd-a925-0bb675c997d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows retrieved: 9,729\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>brent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1987-05-20</td>\n",
              "      <td>18.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1987-05-21</td>\n",
              "      <td>18.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1987-05-22</td>\n",
              "      <td>18.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1987-05-25</td>\n",
              "      <td>18.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1987-05-26</td>\n",
              "      <td>18.63</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  brent\n",
              "0 1987-05-20  18.63\n",
              "1 1987-05-21  18.45\n",
              "2 1987-05-22  18.55\n",
              "3 1987-05-25  18.60\n",
              "4 1987-05-26  18.63"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>brent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9724</th>\n",
              "      <td>2025-09-16</td>\n",
              "      <td>69.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9725</th>\n",
              "      <td>2025-09-17</td>\n",
              "      <td>69.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9726</th>\n",
              "      <td>2025-09-18</td>\n",
              "      <td>67.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9727</th>\n",
              "      <td>2025-09-19</td>\n",
              "      <td>67.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9728</th>\n",
              "      <td>2025-09-22</td>\n",
              "      <td>66.87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           date  brent\n",
              "9724 2025-09-16  69.69\n",
              "9725 2025-09-17  69.19\n",
              "9726 2025-09-18  67.83\n",
              "9727 2025-09-19  67.05\n",
              "9728 2025-09-22  66.87"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "# Packages \n",
        "import io\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# === 1) Define API endpoint and authentication ===\n",
        "APP_TOKEN = \"laCqAPM9Wo1SggEqlGFBAdssN\"  # X-App-Token (public)\n",
        "CSV_ENDPOINT = \"https://agtransport.usda.gov/api/v3/views/b3w8-gxpm/query.csv\"\n",
        "\n",
        "# === 2) Define time range (from 1999 to current UTC date) ===\n",
        "date_from = \"1999-01-01T00:00:00.000\"\n",
        "date_to   = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT23:59:59.999\")\n",
        "\n",
        "# === 3) Request parameters ===\n",
        "params = {\n",
        "    \"select\": \"date, brent\",                   # only these fields\n",
        "    \"where\": f\"date between '{date_from}' and '{date_to}'\",\n",
        "    \"order\": \"date ASC\",\n",
        "}\n",
        "headers = {\"X-App-Token\": APP_TOKEN}\n",
        "\n",
        "# === 4) Download CSV ===\n",
        "resp = requests.get(CSV_ENDPOINT, headers=headers, params=params, timeout=60)\n",
        "resp.raise_for_status()\n",
        "\n",
        "# === 5) Load into pandas ===\n",
        "df = pd.read_csv(io.BytesIO(resp.content))\n",
        "\n",
        "# === 6) Clean data types ===\n",
        "df[\"date\"]  = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
        "df[\"brent\"] = pd.to_numeric(df[\"brent\"], errors=\"coerce\")\n",
        "\n",
        "# Keep only the relevant columns\n",
        "df = df[[\"date\", \"brent\"]]\n",
        "\n",
        "# === 7) Inspect ===\n",
        "print(f\"Rows retrieved: {len(df):,}\")\n",
        "display(df.head(5))\n",
        "display(df.tail(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "JcCds90racOT",
        "outputId": "0c8bc149-673a-43f8-e868-e200101dfbee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing days before ffill: 4,277\n",
            "Missing days after ffill: 0\n",
            "Days filled by ffill: 4,277\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>brent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1987-05-20</td>\n",
              "      <td>18.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1987-05-21</td>\n",
              "      <td>18.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1987-05-22</td>\n",
              "      <td>18.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1987-05-23</td>\n",
              "      <td>18.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1987-05-24</td>\n",
              "      <td>18.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  brent\n",
              "0 1987-05-20  18.63\n",
              "1 1987-05-21  18.45\n",
              "2 1987-05-22  18.55\n",
              "3 1987-05-23  18.55\n",
              "4 1987-05-24  18.55"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>brent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14001</th>\n",
              "      <td>2025-09-18</td>\n",
              "      <td>67.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14002</th>\n",
              "      <td>2025-09-19</td>\n",
              "      <td>67.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14003</th>\n",
              "      <td>2025-09-20</td>\n",
              "      <td>67.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14004</th>\n",
              "      <td>2025-09-21</td>\n",
              "      <td>67.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14005</th>\n",
              "      <td>2025-09-22</td>\n",
              "      <td>66.87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            date  brent\n",
              "14001 2025-09-18  67.83\n",
              "14002 2025-09-19  67.05\n",
              "14003 2025-09-20  67.05\n",
              "14004 2025-09-21  67.05\n",
              "14005 2025-09-22  66.87"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assumes df contains columns 'date' (datetime) and 'brent' (float)\n",
        "df_ff = df.copy().sort_values(\"date\").set_index(\"date\")\n",
        "\n",
        "# Create full daily index from first to last available date\n",
        "full_idx = pd.date_range(df_ff.index.min(), df_ff.index.max(), freq=\"D\")\n",
        "\n",
        "# Reindex to daily frequency (NaN where prices are missing)\n",
        "df_daily = df_ff.reindex(full_idx)\n",
        "\n",
        "# Count missing values before filling\n",
        "missing_before = df_daily[\"brent\"].isna().sum()\n",
        "\n",
        "# Forward-fill (does not fill before the first observation)\n",
        "df_daily[\"brent\"] = df_daily[\"brent\"].ffill()\n",
        "\n",
        "# Optionally drop leading NaNs if the series starts with gaps\n",
        "df_daily = df_daily[df_daily[\"brent\"].notna()]\n",
        "\n",
        "missing_after = df_daily[\"brent\"].isna().sum()\n",
        "filled_days = missing_before - missing_after\n",
        "\n",
        "# Convert index back to column\n",
        "df_daily = df_daily.rename_axis(\"date\").reset_index()\n",
        "\n",
        "# --- Inspect ---\n",
        "print(f\"Missing days before ffill: {missing_before:,}\")\n",
        "print(f\"Missing days after ffill: {missing_after:,}\")\n",
        "print(f\"Days filled by ffill: {filled_days:,}\")\n",
        "\n",
        "display(df_daily.head(5))\n",
        "display(df_daily.tail(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISZOGSFt32GM"
      },
      "source": [
        "## StoxEurope \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqebKVDI3_gI",
        "outputId": "a8057a9d-66f8-406d-c77d-ac8f5256e8e6"
      },
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/bredeespelid/Data_MasterOppgave/main/Variables/StoxEurope/StoxxEuro600.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read as raw text to ensure full control during cleaning\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(URL, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# === 2) Clean column names ===\u001b[39;00m\n\u001b[1;32m     10\u001b[0m raw\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    719\u001b[0m     path_or_buf,\n\u001b[1;32m    720\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    721\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    722\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    723\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "URL = \"https://raw.githubusercontent.com/bredeespelid/Data_MasterOppgave/refs/heads/main/Variables//StoxEurope/StoxxEuro600.csv\"\n",
        "\n",
        "# Les rått som tekst for å sikre full kontroll på rensing\n",
        "raw = pd.read_csv(URL, sep=\",\", dtype=str, encoding=\"utf-8\")\n",
        "\n",
        "# Trim kolonnenavn\n",
        "raw.columns = raw.columns.str.strip()\n",
        "\n",
        "# Parse dato: \"dd.mm.yyyy kl. HH.MM.SS\" -> datetime\n",
        "dt = (\n",
        "    raw[\"Date\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\" kl. \", \" \", regex=False)\n",
        ")\n",
        "date = pd.to_datetime(dt, format=\"%d.%m.%Y %H.%M.%S\", errors=\"coerce\")\n",
        "\n",
        "# Rens Close -> StoxEurope: fjern NBSP/space, bytt komma->punktum, cast til float\n",
        "vals = (\n",
        "    raw[\"Close\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\"\\u00A0\", \"\", regex=False)   # NBSP\n",
        "    .str.replace(\" \", \"\", regex=False)        # vanlige mellomrom\n",
        "    .str.replace(\",\", \".\", regex=False)       # norsk desimal -> engelsk\n",
        "    .replace({\"\": None})\n",
        ")\n",
        "stox = pd.to_numeric(vals, errors=\"coerce\")\n",
        "\n",
        "# Sett sammen til endelig df\n",
        "df = pd.DataFrame({\"Date\": date, \"StoxEurope\": stox}).sort_values(\"Date\").dropna(subset=[\"Date\"]).set_index(\"Date\")\n",
        "\n",
        "# Rask kontroll\n",
        "print(\"Antall NaN i StoxEurope:\", df[\"StoxEurope\"].isna().sum())\n",
        "print(\"Første dato:\", df.index.min().date(), \"Siste dato:\", df.index.max().date())\n",
        "print(df.head(5))\n",
        "print(df.tail(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVboSKNJ4wS_",
        "outputId": "9d825f94-3601-4013-d2cb-9d9d231e6390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antall NaN etter ffill: 0\n",
            "Første dato: 1998-07-17 Siste dato: 2025-09-26\n",
            "                     StoxEurope\n",
            "Date                           \n",
            "1998-07-17 18:00:00      313.83\n",
            "1998-07-20 18:00:00      315.00\n",
            "1998-07-21 18:00:00      313.52\n",
            "1998-07-22 18:00:00      308.13\n",
            "1998-07-23 18:00:00      307.42\n",
            "                     StoxEurope\n",
            "Date                           \n",
            "2025-09-22 18:00:00      553.40\n",
            "2025-09-23 18:00:00      554.95\n",
            "2025-09-24 18:00:00      553.88\n",
            "2025-09-25 18:00:00      550.22\n",
            "2025-09-26 18:00:00      554.52\n"
          ]
        }
      ],
      "source": [
        "# Reindekser til ukedager (uten helligdager) og forward-fill mangler\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Lag ukedagsindeks på dato-nivå og legg på klokkeslett 18:00\n",
        "start = df.index.min().normalize()\n",
        "end = df.index.max().normalize()\n",
        "bidx = pd.bdate_range(start=start, end=end, freq=\"B\") + pd.Timedelta(hours=18)\n",
        "\n",
        "# 2) Reindekser og ffill\n",
        "df_ffill = df.reindex(bidx).ffill()\n",
        "df_ffill.index.name = \"Date\"\n",
        "\n",
        "# 3) Kontroll\n",
        "print(\"Antall NaN etter ffill:\", df_ffill[\"StoxEurope\"].isna().sum())\n",
        "print(\"Første dato:\", df_ffill.index.min().date(), \"Siste dato:\", df_ffill.index.max().date())\n",
        "print(df_ffill.head(5))\n",
        "print(df_ffill.tail(5))\n",
        "\n",
        "# Valgfritt: lagre\n",
        "# df_ffill.to_csv(\"StoxEurope600_ffilled.csv\", index_label=\"Date\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLaQA3BW5xtg"
      },
      "source": [
        "#S&P500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv1YDcFr5zfe",
        "outputId": "4fd8610f-45fb-482f-d68e-c868bf21326a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antall NaN etter ffill: 0\n",
            "Første dato: 1996-11-18 Siste dato: 2025-09-26\n",
            "             SP500\n",
            "Date              \n",
            "1996-11-18  737.02\n",
            "1996-11-19  742.16\n",
            "1996-11-20  742.16\n",
            "1996-11-21  742.72\n",
            "1996-11-22  748.70\n",
            "              SP500\n",
            "Date               \n",
            "2025-09-22  6693.75\n",
            "2025-09-23  6656.92\n",
            "2025-09-24  6637.97\n",
            "2025-09-25  6604.72\n",
            "2025-09-26  6643.70\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "URL_SP = \"https://raw.githubusercontent.com/bredeespelid/Data_MasterOppgave/refs/heads/main/Variables/S%26P500/S%26P.csv\"\n",
        "\n",
        "# --- Hjelpefunksjon for tallrensing ---\n",
        "def _clean_number(s: str) -> str:\n",
        "    s = (s.replace(\"\\u00A0\", \"\")   # NBSP\n",
        "           .replace(\"\\u202F\", \"\")  # smal NBSP\n",
        "           .replace(\" \", \"\"))      # vanlige mellomrom\n",
        "    if \".\" in s and \",\" in s:      # punktum=1000-sep, komma=desimal\n",
        "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
        "    else:\n",
        "        s = s.replace(\",\", \".\")\n",
        "    return s\n",
        "\n",
        "# --- 1) Hent råtekst og parse med regex ---\n",
        "txt = requests.get(URL_SP, timeout=30).text\n",
        "pat = re.compile(r'(\\d{2}\\.\\d{2}\\.\\d{4})\\s+kl\\.\\s+(\\d{2}\\.\\d{2}\\.\\d{2}),\\s*\"?([^\"\\s]+)\"?')\n",
        "rows = [(f\"{d} {t}\", _clean_number(v)) for d, t, v in pat.findall(txt)]\n",
        "\n",
        "# --- 2) Bygg DataFrame ---\n",
        "df_sp = (\n",
        "    pd.DataFrame(rows, columns=[\"Date\", \"SP500\"])\n",
        "      .assign(Date=lambda x: pd.to_datetime(x[\"Date\"], format=\"%d.%m.%Y %H.%M.%S\", errors=\"coerce\").dt.normalize(),\n",
        "              SP500=lambda x: pd.to_numeric(x[\"SP500\"], errors=\"coerce\"))\n",
        "      .dropna(subset=[\"Date\"])\n",
        "      .drop_duplicates(subset=[\"Date\"])\n",
        "      .sort_values(\"Date\")\n",
        "      .set_index(\"Date\")\n",
        ")\n",
        "\n",
        "# --- 3) Reindekser til alle ukedager (uten klokkeslett) og ffill ---\n",
        "bidx_sp = pd.bdate_range(start=df_sp.index.min(), end=df_sp.index.max(), freq=\"B\")\n",
        "df_sp_ffill = df_sp.reindex(bidx_sp).ffill()\n",
        "df_sp_ffill.index.name = \"Date\"\n",
        "\n",
        "# --- 4) Kontroll ---\n",
        "print(\"Antall NaN etter ffill:\", df_sp_ffill[\"SP500\"].isna().sum())\n",
        "print(\"Første dato:\", df_sp_ffill.index.min().date(), \"Siste dato:\", df_sp_ffill.index.max().date())\n",
        "print(df_sp_ffill.head(5))\n",
        "print(df_sp_ffill.tail(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUa4qAfajBj"
      },
      "source": [
        "#DailyCombined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaUJv2F-a9gM",
        "outputId": "cbb866c3-a335-440c-86f1-5c7262afecef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK         Q      d_pi  dI_t  brent    VIX  StoxEurope  \\\n",
            "DATE                                                                      \n",
            "2000-01-31   8.0825  2.030652  0.001817   5.0  27.08  24.95      360.93   \n",
            "2000-02-01   8.0730  2.027659  0.001817   5.0  27.35  23.45      366.71   \n",
            "2000-02-02   8.0175  2.020760  0.001817   5.0  27.15  23.12      371.34   \n",
            "2000-02-03   8.0475  2.024495  0.001817   5.0  27.60  22.01      376.29   \n",
            "2000-02-04   8.0830  2.028897  0.001817   5.0  27.48  21.54      377.37   \n",
            "2000-02-05   8.0830  2.028897  0.001817   5.0  27.48  21.54      377.37   \n",
            "2000-02-06   8.0830  2.028897  0.001817   5.0  27.48  21.54      377.37   \n",
            "2000-02-07   8.0590  2.025923  0.001817   5.0  27.94  22.79      374.20   \n",
            "2000-02-08   8.0720  2.027535  0.001817   5.0  27.61  21.25      382.38   \n",
            "2000-02-09   8.0825  2.028835  0.001817   5.0  27.44  22.90      382.35   \n",
            "2000-02-10   8.0695  2.027225  0.001817   5.0  27.32  23.07      381.61   \n",
            "2000-02-11   8.0395  2.023500  0.001817   5.0  27.82  24.42      381.41   \n",
            "\n",
            "              SP500  \n",
            "DATE                 \n",
            "2000-01-31  1394.46  \n",
            "2000-02-01  1409.28  \n",
            "2000-02-02  1409.12  \n",
            "2000-02-03  1424.96  \n",
            "2000-02-04  1424.37  \n",
            "2000-02-05  1424.37  \n",
            "2000-02-06  1424.37  \n",
            "2000-02-07  1424.14  \n",
            "2000-02-08  1441.39  \n",
            "2000-02-09  1411.86  \n",
            "2000-02-10  1416.84  \n",
            "2000-02-11  1387.11  \n",
            "            EUR_NOK         Q  d_pi  dI_t  brent    VIX  StoxEurope   SP500\n",
            "DATE                                                                       \n",
            "2025-10-16  11.7340  2.427045   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-17  11.7293  2.426644   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-18  11.7293  2.426644   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-19  11.7293  2.426644   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-20  11.7415  2.427684   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-21  11.6693  2.421516   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-22  11.6430  2.419260   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-23  11.5830  2.414093   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-24  11.6185  2.417153   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-25  11.6185  2.417153   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-26  11.6185  2.417153   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "2025-10-27  11.6320  2.418314   0.0   2.0  66.87  16.74      554.52  6643.7\n",
            "\n",
            "Totalt rader: 9,402 | Periode: 2000-01-31 → 2025-10-27 | NaN i brent: 0 | NaN i VIX: 0 | NaN i StoxEurope: 0 | NaN i SP500: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ========= STOXX Europe 600: reindekser til ukedager og ffill =========\n",
        "# Forutsetter at du allerede har 'df' med indeks=DateTime og kolonne 'StoxEurope' (uten NaN).\n",
        "start = df.index.min().normalize()\n",
        "end   = df.index.max().normalize()\n",
        "bidx  = pd.bdate_range(start=start, end=end, freq=\"B\") + pd.Timedelta(hours=18)\n",
        "\n",
        "df_ffill = df.reindex(bidx).ffill()\n",
        "df_ffill.index.name = \"Date\"\n",
        "\n",
        "# Dato-normalisert for «date-only»-align\n",
        "stox_idx = df_ffill.copy()\n",
        "stox_idx.index = stox_idx.index.normalize()\n",
        "\n",
        "# ========= SP500: bruk eksisterende df_sp_ffill (datoindeks uten klokkeslett) =========\n",
        "# df_sp_ffill må finnes fra din tidligere blokk.\n",
        "sp_idx = df_sp_ffill.copy()             # <<< SP500\n",
        "sp_idx.index = sp_idx.index.normalize() # <<< SP500\n",
        "\n",
        "# ========= Brent og VIX: klargjør som før =========\n",
        "# 1) Brent (df_daily: ['date','brent'])\n",
        "brent_idx = (\n",
        "    df_daily.copy()\n",
        "    .assign(date=pd.to_datetime(df_daily[\"date\"], errors=\"coerce\").dt.normalize())\n",
        "    .dropna(subset=[\"date\"])\n",
        "    .set_index(\"date\")\n",
        "    .sort_index()[[\"brent\"]]\n",
        ")\n",
        "\n",
        "# 2) VIX (vix_daily: index=dato, én kolonne som døpes 'VIX')\n",
        "vix_idx = vix_daily.copy()\n",
        "vix_idx.index = pd.to_datetime(vix_idx.index, errors=\"coerce\").normalize()\n",
        "vix_idx.index.name = \"date\"\n",
        "vix_idx.columns = [str(c).strip().upper() for c in vix_idx.columns]\n",
        "if \"VIX\" not in vix_idx.columns and len(vix_idx.columns) == 1:\n",
        "    vix_idx = vix_idx.rename(columns={vix_idx.columns[0]: \"VIX\"})\n",
        "vix_idx = vix_idx.sort_index()[[\"VIX\"]]\n",
        "\n",
        "# ========= Base: sørg for DatetimeIndex =========\n",
        "base = final_with_rates.copy()\n",
        "if not isinstance(base.index, pd.DatetimeIndex):\n",
        "    base.index = pd.to_datetime(base.index, errors=\"coerce\")\n",
        "base = base.sort_index()\n",
        "\n",
        "# Vi aligner på \"dato\" uavhengig av klokkeslett:\n",
        "base_dates = base.index.normalize()\n",
        "\n",
        "# ========= Align alle serier til base-datoer (ffill) =========\n",
        "brent_aligned_bydate = brent_idx.reindex(base_dates, method=\"ffill\")\n",
        "vix_aligned_bydate   = vix_idx.reindex(base_dates,   method=\"ffill\")\n",
        "stox_aligned_bydate  = stox_idx.reindex(base_dates,  method=\"ffill\")\n",
        "sp_aligned_bydate    = sp_idx.reindex(base_dates,    method=\"ffill\")  # <<< SP500\n",
        "\n",
        "# Legg tilbake samme indeks som 'base' (med ev. klokkeslett)\n",
        "brent_aligned = brent_aligned_bydate.set_index(base.index)\n",
        "vix_aligned   = vix_aligned_bydate.set_index(base.index)\n",
        "stox_aligned  = stox_aligned_bydate.set_index(base.index)\n",
        "sp_aligned    = sp_aligned_bydate.set_index(base.index)               # <<< SP500\n",
        "\n",
        "# ========= Rydd overlapp og join =========\n",
        "for col in [\"brent\", \"VIX\", \"vix\", \"StoxEurope\", \"SP500\"]:            # <<< SP500\n",
        "    if col in base.columns:\n",
        "        base = base.drop(columns=[col])\n",
        "\n",
        "final_with_rates = (\n",
        "    base\n",
        "    .join(brent_aligned, how=\"left\")\n",
        "    .join(vix_aligned,   how=\"left\")\n",
        "    .join(stox_aligned,  how=\"left\")\n",
        "    .join(sp_aligned,    how=\"left\")                                   # <<< SP500\n",
        ")\n",
        "\n",
        "# ========= Kolonnerekkefølge (tilpass etter behov) =========\n",
        "pref = [c for c in [\"EUR_NOK\", \"Q\", \"d_pi\", \"dI_t\"] if c in final_with_rates.columns]\n",
        "cols = pref + [c for c in [\"brent\", \"VIX\", \"StoxEurope\", \"SP500\"] if c in final_with_rates.columns]  # <<< SP500\n",
        "final_with_rates = final_with_rates[cols]\n",
        "\n",
        "# ========= Sjekk =========\n",
        "print(final_with_rates.head(12))\n",
        "print(final_with_rates.tail(12))\n",
        "print(\n",
        "    f\"\\nTotalt rader: {len(final_with_rates):,} | \"\n",
        "    f\"Periode: {final_with_rates.index.min().date()} → {final_with_rates.index.max().date()} | \"\n",
        "    f\"NaN i brent: {final_with_rates['brent'].isna().sum():,} | \"\n",
        "    f\"NaN i VIX: {final_with_rates['VIX'].isna().sum():,} | \"\n",
        "    f\"NaN i StoxEurope: {final_with_rates['StoxEurope'].isna().sum():,} | \"  # <<< SP500\n",
        "    f\"NaN i SP500: {final_with_rates['SP500'].isna().sum():,}\"               # <<< SP500\n",
        ")\n",
        "\n",
        "final_with_rates.to_csv(\"final_with_rates.csv\", index_label=\"Date\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K00wKG3-v4l",
        "outputId": "911919d5-3f89-4bf9-be47-0d05be2183ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            EUR_NOK         Q      d_pi  dI_t     brent       VIX  StoxEurope  \\\n",
            "DATE                                                                            \n",
            "2000-01-31   8.0825  2.030652  0.001817   5.0  3.298795  3.216874    5.888684   \n",
            "2000-02-01   8.0730  2.027659  0.001817   5.0  3.308717  3.154870    5.904571   \n",
            "2000-02-02   8.0175  2.020760  0.001817   5.0  3.301377  3.140698    5.917118   \n",
            "2000-02-03   8.0475  2.024495  0.001817   5.0  3.317816  3.091497    5.930360   \n",
            "2000-02-04   8.0830  2.028897  0.001817   5.0  3.313458  3.069912    5.933226   \n",
            "...             ...       ...       ...   ...       ...       ...         ...   \n",
            "2025-10-23  11.5830  2.414093  0.000000   2.0  4.202750  2.817801    6.318103   \n",
            "2025-10-24  11.6185  2.417153  0.000000   2.0  4.202750  2.817801    6.318103   \n",
            "2025-10-25  11.6185  2.417153  0.000000   2.0  4.202750  2.817801    6.318103   \n",
            "2025-10-26  11.6185  2.417153  0.000000   2.0  4.202750  2.817801    6.318103   \n",
            "2025-10-27  11.6320  2.418314  0.000000   2.0  4.202750  2.817801    6.318103   \n",
            "\n",
            "               SP500  \n",
            "DATE                  \n",
            "2000-01-31  7.240263  \n",
            "2000-02-01  7.250834  \n",
            "2000-02-02  7.250721  \n",
            "2000-02-03  7.261899  \n",
            "2000-02-04  7.261485  \n",
            "...              ...  \n",
            "2025-10-23  8.801424  \n",
            "2025-10-24  8.801424  \n",
            "2025-10-25  8.801424  \n",
            "2025-10-26  8.801424  \n",
            "2025-10-27  8.801424  \n",
            "\n",
            "[9402 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ny df: behold alt, men logg eksogene i kopien\n",
        "final_with_rates_log = final_with_rates.copy()\n",
        "\n",
        "# Eksogene som skal logges (bruk bare de som faktisk finnes)\n",
        "exog_cols = [c for c in [\"brent\", \"VIX\", \"StoxEurope\", \"SP500\"] if c in final_with_rates_log.columns]\n",
        "\n",
        "# Sørg for numeriske typer\n",
        "final_with_rates_log[exog_cols] = final_with_rates_log[exog_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Logg kun for positive verdier (<=0 -> NaN)\n",
        "final_with_rates_log[exog_cols] = np.log(final_with_rates_log[exog_cols].where(final_with_rates_log[exog_cols] > 0))\n",
        "\n",
        "\n",
        "print(final_with_rates_log)\n",
        "\n",
        "# Lagre til CSV\n",
        "final_with_rates_log.to_csv(\"final_with_rates_log.csv\", index_label=\"Date\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
